{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fjavadi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\fjavadi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
    "\n",
    "import os,json\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Conv1D,MaxPooling1D,Embedding\n",
    "from keras.models import load_model, Model\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import nltk\n",
    "user = 'dev_ds'\n",
    "pwd = 'akBefvuIj4'\n",
    "from base64 import b64decode\n",
    "conn_str = 'mssql+pyodbc://%s:%s@repdb1irv.iherb.net/iHerb_Shop?driver=ODBC+Driver+13+for+SQL+Server'\n",
    "engine = sa.create_engine(conn_str % ('dev_ds', b64decode('YWtCZWZ2dUlqNA==').decode()))\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30070\n",
      "11437\n"
     ]
    }
   ],
   "source": [
    "#Removing the main first level categories\n",
    "#ParentCategoryID=8120 : Brands, HealthTopics=\n",
    "p='inventoryvelocity'\n",
    "query3='''\n",
    "SELECT A.ProductID as ProductID, A.ParentCategoryID as CategoryID, B.level as level, B.d as d, B.Name as Name \n",
    "FROM `inventoryvelocity.input.product_parent_categories_relation` AS A\n",
    "Inner join `rec-serv.AutoSuggestion.category_data` AS B on A.ParentCategoryID=B.CategoryID \n",
    "Where B.VisibleToCustomer=True and B.CategoryID not in (1475,1855,2089,2203,2236,2320,2992,3213,100477,100483,2430,2493,2465,8120)\n",
    "and B.ParentCategoryID not in (8120,2430)\n",
    "'''\n",
    "products_OldCategories=pd.read_gbq(query3,p,dialect='standard')\n",
    "\n",
    "product_attr = pd.read_sql(''' \n",
    "SELECT [ProductID]\n",
    "      ,[DisplayName]\n",
    "      ,[Ingredients]\n",
    "      ,[Description]\n",
    "      ,[SupplementFacts]\n",
    "      ,[ServingSize]\n",
    "      ,[PackageQuantity]\n",
    "      ,[SuggestedUse]\n",
    "      ,[Keywords]\n",
    "  FROM [iHerb_Shop].[shopservice].[TBL_Products]\n",
    "  where Status in (0,3,4,6)\n",
    "''', engine)\n",
    "#product_attr.set_index('ProductID' , inplace=True)\n",
    "print(len(product_attr))\n",
    "products_OldCategories=products_OldCategories[products_OldCategories['ProductID'].isin(product_attr.index.values)]\n",
    "print(products_OldCategories['ProductID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', ' ', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isnumeric() or isinstance(word, float):\n",
    "            new_word=' '\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            word1=word\n",
    "            new_words.append(word1)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if(lemma!=\" \"):\n",
    "            lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(s):\n",
    "    s = re.sub(r'[^\\w\\s]',' ',s)\n",
    "    words = nltk.word_tokenize(s)  \n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words= lemmatize_verbs(words)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ProductID', 'DisplayName', 'CategoryID'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "query3='''\n",
    "SELECT A.ProductID as ProductID, A.ParentCategoryID as CategoryID, B.level as level, B.d as d, B.Name as Name \n",
    "FROM `inventoryvelocity.input.product_parent_categories_relation` AS A\n",
    "Inner join `rec-serv.AutoSuggestion.category_data` AS B on A.ParentCategoryID=B.CategoryID\n",
    "and B.ParentCategoryID in (1855) and CategoryID not in (101129,100865,101072,8736)\n",
    "'''\n",
    "products_supplements=pd.read_gbq(query3,p,dialect='standard')\n",
    "\n",
    "products_supplements=products_supplements[products_supplements['ProductID'].isin(product_attr.index.values)]\n",
    "list_supplements_products=products_supplements['ProductID'].unique().tolist()\n",
    "\n",
    "result_categories = pd.merge( product_attr, products_supplements, on='ProductID', how='inner')\n",
    "#product_attr.set_index('ProductID' , inplace=True)\n",
    "result_categories['DisplayName']=result_categories['DisplayName'].apply(lambda x: normalize(x))\n",
    "result_categories =result_categories[~ result_categories.isnull()]\n",
    "result_categories = result_categories[~ result_categories['DisplayName'].isnull()]\n",
    "result_categories=result_categories.groupby(['ProductID','DisplayName'],as_index=False)['CategoryID'].apply(list).reset_index()\\\n",
    ".rename(columns={0:'CategoryID'})\n",
    "print(result_categories.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1476   1542   1694   1800   1930   3282   8738   8741 100727 100804\n",
      " 100821 100858 100861 100945 101005]\n"
     ]
    }
   ],
   "source": [
    "Category_results = result_categories['CategoryID'].values.tolist() \n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_categories=mlb.fit_transform(Category_results)\n",
    "print(mlb.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CategoryID_matrix = result_categories.as_matrix(columns=[result_categories[3:]])\n",
    "\n",
    "#print(result_categories.iloc[:,[2]].as_matrix())\n",
    "\n",
    "#categoryId_matrix = result_categories.iloc[:,[2]].as_matrix()\n",
    "#X_train, X_test, y_train, y_test =train_test_split( result_categories['DisplayName'], result_categories['CategoryID'], test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test =train_test_split( result_categories['DisplayName'], binary_categories , test_size=0.33, random_state=42)\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer.fit(X_train)\n",
    "vectorizer.fit(X_test)\n",
    "x_train = vectorizer.transform(X_train)\n",
    "x_test= vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.7083764219234746\n"
     ]
    }
   ],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "classifier = BinaryRelevance(GaussianNB(),require_dense = [True, True]\n",
    ")\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.3360910031023785\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# initialize classifier chains multi-label classifier\n",
    "classifier = ClassifierChain(LogisticRegression())\n",
    "# Training logistic regression model on train data\n",
    "classifier.fit(x_train, y_train)\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.7218200620475698\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.70      0.77       138\n",
      "          1       0.89      0.95      0.92       106\n",
      "          2       0.78      0.82      0.80        71\n",
      "          3       0.54      0.98      0.70       163\n",
      "          4       1.00      0.48      0.65        21\n",
      "          5       1.00      0.48      0.65        48\n",
      "          6       1.00      0.45      0.62        33\n",
      "          7       0.77      0.63      0.69        98\n",
      "          8       0.95      0.55      0.69        77\n",
      "          9       1.00      0.39      0.56        59\n",
      "         10       1.00      0.48      0.65        25\n",
      "         11       0.77      0.82      0.79       111\n",
      "         12       1.00      0.39      0.56        36\n",
      "         13       1.00      0.38      0.55        21\n",
      "         14       0.98      0.80      0.88        59\n",
      "\n",
      "avg / total       0.84      0.71      0.73      1066\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using Label Powerset\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "# initialize label powerset multi-label classifier\n",
    "classifier = LabelPowerset(LogisticRegression())\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions))\n",
    "# using Label Powerset\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6026694045174538\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "classifier_new = MLkNN(k=10)\n",
    "# Note that this classifier can throw up errors when handling sparse matrices.\n",
    "x_train = lil_matrix(x_train).toarray()\n",
    "y_train = lil_matrix(y_train).toarray()\n",
    "x_test = lil_matrix(x_test).toarray()\n",
    "# train\n",
    "classifier_new.fit(x_train, y_train)\n",
    "# predict\n",
    "predictions_new = classifier_new.predict(x_test)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,predictions_new))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
